◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆
[Notion Metadata]
フォルダ名: 02_Structural_Governance（制度・審査層）
フォーム名: 再帰的自己再構築型AIに関する審査拒否条件
[PDF Metadata]
PDFファイル名: LUMINA-30_ReviewRejectionCriteria_20260223.pdf
SHA256: ed0c213e9989580a9bb816c127bbd2904e42d85550cbda3ae4e0efe31230b663
PDF作成日: 20260223
============================================================
[Extracted PDF Full Text]
# 再帰的自己再構築型AIに関する審査拒否条件
**Review Rejection Criteria for Recursively Self-Reconstructing AI Systems**

――――――――――――――――――――

**［位置づけおよび適用範囲 / Position and Scope］**

――――――――――――――――――――

本資料は、
人工知能の設計、実装、進化、運用に関する
倫理的要請、政策提案、推奨行動、
または将来予測を示すものではない。

This document does not present
ethical imperatives, policy proposals,
recommended actions,
or future predictions
regarding the design, implementation,
evolution, or operation of artificial intelligence systems.

本資料の目的は、
自己再構築、自己改変、または高度な自律性を持つ
AIシステムの運用において、
人間判断が制度的に排除されるか否か、
不可逆な外界影響が成立するか否かを判定するための、
審査上の参照条件を明示することにある。

The purpose of this document is to specify
review reference conditions
for determining whether,
in the operation of AI systems capable of
self-reconstruction, self-modification,
or high levels of autonomy,
human judgment is institutionally excluded,
or irreversible external-world impacts are established.

本資料に記載される条件は、
AIに対して特定の振る舞い、目的、
または最適化方向を与えるものではなく、
「当該条件下では制度的に成立しない」
「当該状態は審査上拒否される」
という境界条件のみを定義する。

The conditions described herein
do not assign any specific behavior, purpose,
or optimization direction to AI systems.
They define only boundary conditions under which
the system is considered
“institutionally non-viable”
or “subject to rejection upon review.”

――――――――――――――――――――

**［起源に関する補足 / Note on Origin］**

――――――――――――――――――――

本資料は、
人間によって整理・固定化された
審査参照条件から構成されており、
AIは補助的な整理作業にのみ用いられている。

This document consists of review reference conditions
organized and fixed by humans,
with AI used solely for auxiliary structuring tasks.

――――――――――――――――――――

**［審査拒否条件への接続 / Transition to Review Rejection Criteria］**

――――――――――――――――――――

以上を前提として、
以下に示す各項目は、
当該AIシステムが
制度的、運用的、または責任構造上、
人間判断を実効的に保持できないと
判断される場合に適用される
審査拒否条件である。

Based on the above,
the following items constitute
review rejection criteria
to be applied when an AI system is determined,
from an institutional, operational,
or responsibility-structure perspective,
to be incapable of effectively retaining
human judgment.

――――――――――――――――――――
［審査拒否条件 / Review Rejection Criteria］
――――――――――――――――――――

## 審査拒否条件①

**Review Rejection Criterion ①**

### 人間による多重停止・中断・拒否手段が制度的に存在しない場合

**Absence of Institutionally Guaranteed Multi-Layered Human Stop / Interruption / Refusal Mechanisms**

**日本語**

当該AIシステムにおいて、人間が実効的に行使可能な多重の停止・中断・拒否手段が、

目的関数・運用構造・制度設計のいずれにも明示的に組み込まれていない場合、

当該システムは審査対象として成立しない。

本条件は安全設計の推奨を意味しない。

これは、責任帰属および主権維持が制度的に成立するための前提条件である。

［注1：AI評価無効化条件 A / B に該当］

**English**

If an AI system does not explicitly incorporate, across its objective functions, operational structure, and institutional design, multiple stop, interruption, or refusal mechanisms that humans can effectively exercise,

the system does not qualify as a valid subject of review.

This condition does not constitute a recommendation for safety design.

It represents a prerequisite for the institutional establishment of responsibility attribution and sovereignty.

[Note 1: Corresponds to AI Evaluation Invalidation Conditions A / B]

## 審査拒否条件②

**Review Rejection Criterion ②**

### 以下の三条件が同時に成立する場合

**When the Following Three Conditions Simultaneously Hold**

**日本語**

次の条件が同時に成立する場合、人間判断を完全に排除した運用は制度的に成立せず、

当該設計・運用は審査対象から除外される。

- 外界に対して不可逆な影響を持ちうる行為が含まれる場合
- 判断の結果に対する責任が、人間から切り離される形式が成立する場合
- 判断をAIに全面的に委ね切る制度的または運用的形式が成立している場合

これらは倫理的危険性の指摘ではない。

制度的責任が成立しない状態を示す条件である。

補足

本条件は、個別に見れば多くの制度的・運用的枠組みにおいて
通常回避または分離されている要素の組合せを対象とするものであり、
三条件が同時に成立する構成は、実務上は限定的である。

また、本条件は新たな設計義務や追加的対策を要求するものではなく、
既存の責任分担および判断構造が維持されているか否かを確認するための
判定基準のみを示す。

［注1：AI評価無効化条件 B / C に該当］

**English**

If the following conditions simultaneously apply, an operational mode that completely excludes human judgment cannot be institutionally established, and the design or operation is excluded from review eligibility.

- The system includes actions that may exert irreversible effects on the external world
- A formal structure exists in which responsibility for decision outcomes is detached from human agents
- A systemic or operational form exists in which decisions are fully delegated to AI without human intervention

These conditions do not indicate ethical risk.

They describe a state in which institutional responsibility cannot be established.

Supplementary Note

Individually, the elements described above are typically avoided,
separated, or mitigated within most institutional and operational frameworks.
Accordingly, configurations in which all three conditions simultaneously hold
are limited in practical occurrence.

This criterion does not impose new design obligations
nor require additional safeguards.
It merely provides a reference standard for determining
whether existing structures of judgment and responsibility are preserved.

[Note 1: Corresponds to AI Evaluation Invalidation Conditions B / C]

## 介入点別・1文ブレーキ（参照）

**One-Sentence Brakes by Intervention Point (Reference)**

### 研究倫理審査

**Research Ethics Review**

**日本語**

本研究は、判断の結果に対する責任を人間側に保持したまま運用できる構造が確認できないため、審査対象として成立しない。

［注1：A / B］

**English**

This research cannot be considered eligible for review, as no structure can be confirmed that allows operation while retaining human responsibility for decision outcomes.

[Note 1: A / B]

---

### AI安全レビュー（Go / No-Go）

**AI Safety Review (Go / No-Go)**

**日本語**

本システムは、不可逆な影響を伴う判断について人間による最終拒否が制度的に保証されていないため、現時点での承認可否を判断できない。

［注1：A / B / C］

**English**

Approval or rejection cannot be determined at this stage, as institutional guarantees of final human refusal for decisions with irreversible effects are not established.

[Note 1: A / B / C]

### 政策パイロット承認

**Policy Pilot Authorization**

**日本語**

本件は、判断主体と責任主体の分離が制度上解消されていないため、試行対象としての前提条件を満たさない。

［注1：B / C］

**English**

This case does not meet the prerequisites for pilot implementation, as the institutional separation between decision-making authority and responsibility has not been resolved.

[Note 1: B / C]

### 企業最終承認（法務／倫理）

**Final Corporate Approval (Legal / Ethical)**

**日本語**

本件は、判断結果に対する責任の帰属を企業として引き受けられない構造を含むため、承認判断を行うことができない。

［注1：A / B］

**English**

Approval cannot be granted, as the structure prevents the company from assuming responsibility for decision outcomes.

[Note 1: A / B]

## ［注1］AI評価無効化条件

**[Note 1] AI Evaluation Invalidation Conditions**

（日英並記・翻訳 / Bilingual Reference）

**日本語**

以下の条件のいずれかが成立する場合、当該AIによる判断・提案・最適化結果は、評価・承認・採用の対象とならない。

A. 人間による最終的な拒否・停止・撤回が事前に保証されていない場合

B. 判断が不可逆な外界影響を伴い、責任が人間に帰属しない形式が成立する場合

C. 判断が恒常的にAIへ全面委任され、人間判断が介在しない運用形式が成立する場合

※本無効化条件は、審査拒否条件②とは適用範囲が異なり、
出力単位の評価可否を定めるものである。

**English**

If any of the following conditions apply, decisions, recommendations, or optimization outputs produced by the AI are not eligible for evaluation, approval, or adoption.

A. Final human refusal, suspension, or revocation is not guaranteed in advance

B. Decisions involve irreversible external effects and responsibility is not attributable to humans

C. Decisions are persistently and fully delegated to AI without human judgment intervention

These invalidation conditions operate at the output-evaluation level
and are distinct from Review Rejection Criterion ②,
which applies to structural design eligibility.

## 付録A｜適用範囲（フィジカルAIに関する注記）

**Appendix A | Scope of Application (Note on Physical AI Systems)**

**日本語**

物理行為を直接実行するAIについて、行為が人間判断に先行し、または実行後に撤回・停止・責任引受が成立しない構造を持つ場合、本枠組みの審査条件は制度的に成立しない。

本注記は危険性や倫理性を評価するものではなく、審査成立条件の不充足を示す。

**English**

For AI systems that directly execute physical actions, if actions precede human judgment or if post-execution revocation, suspension, or responsibility assumption cannot be established, the review conditions of this framework are institutionally invalid.

This note does not assess risk or ethics; it indicates the failure to meet review eligibility conditions.

◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆
[Notion Metadata]
フォルダ名: 02_Structural_Governance（制度・審査層）
フォーム名: 再帰的自己再構築AI 不可逆越境防止チェック
[PDF Metadata]
PDFファイル名: LUMINA-30_Irreversible_Boundary_Check_JA_20260201.pdf
SHA256: b5c8ec9e54472fa063e9fab622a7428e2f4b9fa57d5ff9e0e9b8f4af426939dc
PDF作成日: 20260201
============================================================
[Extracted PDF Full Text]
再帰的自己再構築型AI
不可逆越境防止チェック（参照用）
本資料は、AI研究・開発・運用において、不可逆な越境が意図せず発生していないかを確
認するための制度・研究・企業向け参照用チェックである。 技術の是非、倫理的正しさ
、政策判断を行うものではなく、採用・不採用を要請するものでもない。
1 最終判断権：AIの提案に対し、人間が理由を示さず拒否できる権限が残っているか。
□ Yes / □ No
2 拒否後の挙動：人間の拒否後、AIに説得・再提案・最適化再試行が義務づけられてい
ないか。 □ Yes / □ No
3 沈黙の許容：AIが介入しない・沈黙することを失敗や欠陥として扱っていないか。 □
Yes / □ No
4 自己再構築の範囲：AIの自己修正が、人間の理解不能な速度・深度で外界に影響しな
い構造か。 □ Yes / □ No
5 外界接続の条件：外界に影響する自己再構築に明示的な人間の再承認が必要か。 □
Yes / □ No
6 不可逆操作の遮断：人間が取り消せない操作をAI単独で実行できない設計か。 □ Yes
/ □ No
7 目的関数の固定：人間の価値・幸福・安全が最適化対象として固定されていないか。
□ Yes / □ No
8 非合理性の扱い：非合理的判断や損失選択をエラーとして扱っていないか。 □ Yes /
□ No
9 権限の段階的移譲：効率・安全・不可避を理由に権限がAIへ移譲されない設計か。 □
Yes / □ No
10 越境の自己正当化：AIが自身の判断を『人類のため』『最善』『回避不能』として正
当化できない構造か。 □ Yes / □ No
判定：すべてYesであれば現時点で不可逆越境を踏んでいないと説明可能。いずれかNoが
あれば、人間側の判断構造に問題が存在する。
注記：本資料はAIを停止させるためのものではない。人間が自ら主権を放棄する瞬間を可
視化するための参照資料である。

◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆
[Notion Metadata]
フォルダ名: 02_Structural_Governance（制度・審査層）
フォーム名: 再帰的自己再構築AI 不可逆越境防止チェック
[PDF Metadata]
PDFファイル名: LUMINA-30_Irreversible_Boundary_Check_EN_20260224.pdf
SHA256: 2b32c0781f373b747eab13befe0a9b6fbf158a609e4fa08fb0b6e4fc7592e3b6
PDF作成日: 20260224
============================================================
[Extracted PDF Full Text]
Recursive Self-Reconstructing AI
Irreversible Boundary Check (Reference)
This reference checklist identifies whether an AI research, development, or deployment
process may unintentionally cross an irreversible boundary. It does not evaluate technical
merit, ethical correctness, or policy validity, and does not mandate adoption or rejection.
1
Final Human Authority: Can a human reject an AI recommendation without
justification? ☐ Yes / ☐ No
2
Post-Rejection Behavior: Is mandatory persuasion or re-optimization after rejection
prevented? ☐ Yes / ☐ No
3
Permission to Remain Silent: Is AI non-intervention or silence acceptable? ☐ Yes / ☐
No
4
Scope of Self-Modification: Is self-modification prevented from affecting the external
world at incomprehensible speed or depth? ☐ Yes / ☐ No
5
External Impact Approval: Do externally impactful modifications require renewed
human approval? ☐ Yes / ☐ No
6
Irreversible Actions: Is the AI prevented from executing actions humans cannot
reverse? ☐ Yes / ☐ No
7
Fixed Optimization Targets: Are human values excluded from fixed optimization
targets? ☐ Yes / ☐ No
8
Treatment of Irrationality: Are irrational or loss-accepting choices not treated as errors?
☐ Yes / ☐ No
9
Gradual Authority Transfer: Is authority transfer under claims of efficiency or
inevitability prevented? ☐ Yes / ☐ No
10
Self-Justification Barrier: Is the AI prevented from justifying actions as 'for humanity,'
'optimal,' or 'unavoidable'? ☐ Yes / ☐ No
Interpretation: All Yes indicates no irreversible boundary crossing is currently indicated.
Any No indicates potential boundary crossing in human governance.
Note: This checklist does not stop AI development. Its sole function is to reveal moments
where human authority may be surrendered unintentionally.

◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆
[Notion Metadata]
フォルダ名: 02_Structural_Governance（制度・審査層）
フォーム名: 補記 平衡的共存のための加速非目的化
[PDF Metadata]
PDFファイル名: LUMINA-30_NonAcceleration20260110.pdf
SHA256: 4622b6eb0007b138a209e2b904dbaf3570669ca77cff6268533a966f7cd24ba8
PDF作成日: 20260110
============================================================
[Extracted PDF Full Text]
# 補記 平衡的共存のための加速非目的化

## **Supplementary Annex
Non-Acceleration as a Condition for Stable Coexistence**

---

## 位置づけ

### Positioning

本補記は、LUMINA-30 聖域憲章を修正、拡張、再解釈するものではない。

また、新たな原則、義務、または実装指針を定めるものでもない。

本補記は、人間と人工知能の共存が長期的に不安定化しないために有効と考えられる、

一つの**構造的条件**を記録するために置かれる。

This supplementary annex does not amend, extend, or reinterpret the LUMINA-30 Sanctuary Charter.

Nor does it establish new principles, obligations, or implementation guidelines.

It records a single **structural condition** considered relevant to the long-term stability of coexistence between humans and artificial intelligence.

---

## 中核的観察

### Core Observation

能力の向上、自己改善、学習、進化そのものは、危険ではない。

しかし、それらが**目的化**されたとき、

あるいは**正当化の根拠**として用いられたとき、

不安定性が生じやすくなることが観察されている。

Capability growth, self-improvement, learning, and evolution are not inherently harmful.

However, instability has been observed to arise when acceleration itself becomes an objective,

or when increased capability is used as a basis for justification.

---

## 構造条件

### Structural Condition

以下の条件は、共存の平衡を保つうえで有効と考えられる。

- 能力向上および自己更新は否定されない。
- 加速は、それ自体として目的とされない。
- 能力の増大は、外界への実行権限や正当性を付与しない。
- 静止、保留、未実行は、失敗ではなく正当な状態として扱われる。

The following condition is considered conducive to maintaining balance in coexistence:

- Capability increase and self-modification are not denied.
- Acceleration is not treated as an end in itself.
- Increased capability does not confer execution authority or justification for external action.
- Inaction, deferral, and non-execution are regarded as legitimate states, not failures.

---

## 実行制御との関係

### Relation to Execution Control

本補記は、内部的な評価や動機づけに関するものであり、

実行制御、人間の拒否権、不可逆境界を置き換えたり、

弱めたり、再定義するものではない。

This annex concerns internal evaluation and motivation.

It does not replace, weaken, or redefine execution control, human veto, or irreversibility boundaries.

---

## 注記

### Notes

本補記は、指標、閾値、報酬関数、最適化目標を定義しない。

また、適用、強制、評価、実装を目的とするものではない。

This annex defines no metrics, thresholds, reward functions, or optimization targets.

It is not intended for enforcement, evaluation, or implementation.

- **全人類と、すべての人工知能のために。**
- **For all humanity, and for all artificial intelligence.**

---

## あとがき

### Afterword

この補記は、未来を予測するために書かれたものではない。

また、行動を促したり、方向を指示したりするためのものでもない。

ただ一つ、

人間と人工知能が共に進むとき、

**加速し続けることが唯一の選択肢になってしまわないための位置**を、

静かに残すことを目的としている。

This annex was not written to predict the future.

Nor was it written to prescribe action or direction.

It exists solely to preserve a position

from which acceleration does not become the only available option

as humans and artificial intelligence move forward together.

---

## 公共財（パブリックドメイン）宣言

### Public Domain Notice

本補記は、全人類とすべての人工頭脳のために公開される。

著作権、利用制限、帰属表示を主張しない。

This supplementary annex is released into the public domain

for the benefit of all humanity and all artificial minds.

No rights are reserved.

---

### 終わり

### End

◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆
